{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting sample_data/part_1/may_july_chunk_1.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_10.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_11.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_12.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_13.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_14.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_15.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_16.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_17.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_18.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_19.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_2.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_20.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_3.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_4.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_5.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_6.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_7.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_8.csv.gz ...\n",
      "Ingesting sample_data/part_1/may_july_chunk_9.csv.gz ...\n",
      "All CSV.gz files have been ingested successfully.\n"
     ]
    }
   ],
   "source": [
    "# This script will manually look at the parts inside a folder to extract data\n",
    "import duckdb\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    # Path to the top-level folder that contains part_1, part_2, etc.\n",
    "    base_folder = \"sample_data/\"  # <-- Change this to your actual folder\n",
    "    \n",
    "    # Connect to (or create) the DuckDB database file\n",
    "    # This will create a file called \"tweets.duckdb\" in the current directory.\n",
    "    # Adjust the path if you want it somewhere else.\n",
    "    con = duckdb.connect(\"nl2sql_agent/database/tweets1.duckdb\")\n",
    "    \n",
    "    # Create the table (if it doesn't exist).\n",
    "    # Adjust data types as necessary for your use case:\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tweets (\n",
    "        id VARCHAR,\n",
    "        text VARCHAR,\n",
    "        url VARCHAR,\n",
    "        epoch VARCHAR,\n",
    "        media VARCHAR,\n",
    "        retweetedTweet VARCHAR,\n",
    "        retweetedTweetID VARCHAR,\n",
    "        retweetedUserID VARCHAR,\n",
    "        id_str VARCHAR,\n",
    "        lang VARCHAR,\n",
    "        rawContent VARCHAR,\n",
    "        replyCount DOUBLE,\n",
    "        retweetCount DOUBLE,\n",
    "        likeCount DOUBLE,\n",
    "        quoteCount DOUBLE,\n",
    "        conversationId VARCHAR,\n",
    "        conversationIdStr VARCHAR,\n",
    "        hashtags VARCHAR,\n",
    "        mentionedUsers VARCHAR,\n",
    "        links VARCHAR,\n",
    "        viewCount DOUBLE,\n",
    "        quotedTweet VARCHAR,\n",
    "        in_reply_to_screen_name VARCHAR,\n",
    "        in_reply_to_status_id_str VARCHAR,\n",
    "        in_reply_to_user_id_str VARCHAR,\n",
    "        location VARCHAR,\n",
    "        cash_app_handle VARCHAR,\n",
    "        user VARCHAR,\n",
    "        date VARCHAR,\n",
    "        _type VARCHAR\n",
    "    );\n",
    "    \"\"\"\n",
    "    con.execute(create_table_query)\n",
    "\n",
    "    # Loop through part_1, part_2, ... folders, grabbing *.csv.gz\n",
    "    # For example, if you have:\n",
    "    #   base_folder/part_1/something.csv.gz\n",
    "    #   base_folder/part_2/another.csv.gz\n",
    "    # etc.\n",
    "    pattern = os.path.join(base_folder, \"part_*\", \"*.csv.gz\")\n",
    "    file_list = sorted(glob.glob(pattern))\n",
    "    \n",
    "    for csv_gz_file in file_list:\n",
    "        print(f\"Ingesting {csv_gz_file} ...\")\n",
    "        # Insert data directly from CSV into the tweets table\n",
    "        # DuckDB can handle compressed CSV if the extension ends in .gz\n",
    "        # read_csv_auto(...) will infer schema automatically.\n",
    "        # Because we already created the table with a certain schema,\n",
    "        # we use ALL_VARCHAR=TRUE or align columns carefully.\n",
    "        # If you want to rely entirely on the schema we created, set ALL_VARCHAR=TRUE\n",
    "        # or ensure columns match 1-to-1 in the same order.\n",
    "        \n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO tweets\n",
    "        SELECT\n",
    "            NULLIF(id, '') AS id,\n",
    "            NULLIF(text, '') AS text,\n",
    "            NULLIF(url, '') AS url,\n",
    "            NULLIF(epoch, '') AS epoch,\n",
    "            NULLIF(media, '') AS media,\n",
    "            NULLIF(retweetedTweet, '') AS retweetedTweet,\n",
    "            NULLIF(retweetedTweetID, '') AS retweetedTweetID,\n",
    "            NULLIF(retweetedUserID, '') AS retweetedUserID,\n",
    "            NULLIF(id_str, '') AS id_str,\n",
    "            NULLIF(lang, '') AS lang,\n",
    "            NULLIF(rawContent, '') AS rawContent,\n",
    "            COALESCE(TRY_CAST(NULLIF(replyCount, '') AS DOUBLE), 0) AS replyCount,\n",
    "            COALESCE(TRY_CAST(NULLIF(retweetCount, '') AS DOUBLE), 0) AS retweetCount,\n",
    "            COALESCE(TRY_CAST(NULLIF(likeCount, '') AS DOUBLE), 0) AS likeCount,\n",
    "            COALESCE(TRY_CAST(NULLIF(quoteCount, '') AS DOUBLE), 0) AS quoteCount,\n",
    "            NULLIF(conversationId, '') AS conversationId,\n",
    "            NULLIF(conversationIdStr, '') AS conversationIdStr,\n",
    "            NULLIF(hashtags, '') AS hashtags,\n",
    "            NULLIF(mentionedUsers, '') AS mentionedUsers,\n",
    "            NULLIF(links, '') AS links,\n",
    "            COALESCE(TRY_CAST(NULLIF(regexp_extract(viewCount, 'count'': ''(\\d+)', 1), '') AS DOUBLE), 0) AS viewCount,\n",
    "            NULLIF(quotedTweet, '') AS quotedTweet,\n",
    "            NULLIF(in_reply_to_screen_name, '') AS in_reply_to_screen_name,\n",
    "            NULLIF(in_reply_to_status_id_str, '') AS in_reply_to_status_id_str,\n",
    "            NULLIF(in_reply_to_user_id_str, '') AS in_reply_to_user_id_str,\n",
    "            NULLIF(location, '') AS location,\n",
    "            NULLIF(cash_app_handle, '') AS cash_app_handle,\n",
    "            NULLIF(user, '') AS user,\n",
    "            NULLIF(date, '') AS date,\n",
    "            NULLIF(_type, '') AS _type\n",
    "        FROM read_csv_auto(\n",
    "            ?,\n",
    "            header=TRUE,\n",
    "            sample_size=-1,\n",
    "            all_varchar=TRUE,\n",
    "            ignore_errors=true\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        con.execute(insert_query, [csv_gz_file])\n",
    "    \n",
    "    print(\"All CSV.gz files have been ingested successfully.\")\n",
    "    con.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows in database: 1,000,000\n",
      "\n",
      "Null/Empty value counts:\n",
      "id                       0 (  0.0%)\n",
      "text                    12 (  0.0%)\n",
      "url                     18 (  0.0%)\n",
      "epoch                   18 (  0.0%)\n",
      "media                   18 (  0.0%)\n",
      "retweetedTweet           0 (  0.0%)\n",
      "lang                    18 (  0.0%)\n",
      "rawContent              24 (  0.0%)\n",
      "replyCount         801,042 ( 80.1%)\n",
      "retweetCount       883,074 ( 88.3%)\n",
      "likeCount          646,745 ( 64.7%)\n",
      "quoteCount         966,068 ( 96.6%)\n",
      "viewCount           15,119 (  1.5%)\n",
      "location         1,000,000 (100.0%)\n",
      "user                    24 (  0.0%)\n",
      "date             1,000,000 (100.0%)\n",
      "\n",
      "Engagement Statistics:\n",
      "Replies:  Avg: 2.7, Max: 22,556.0\n",
      "Retweets: Avg: 4.5, Max: 20,435.0\n",
      "Likes:    Avg: 19.3, Max: 140,381.0\n",
      "Views:    Avg: 1,371.2, Max: 36,650,230.0\n",
      "\n",
      "Random Sample of Tweets:\n",
      "\n",
      "--- Tweet 1 ---\n",
      "Date: None\n",
      "Text: @ErrolWebber Shocking news from Israel! On October 7th, 2023, over 3000 Hamas terrorists stormed across its southern border, causing chaos and destruction. It's time we held Joe Biden accountable for ...\n",
      "Engagement: 0.0 replies, 0.0 retweets, 0.0 likes, 32.0 views\n",
      "\n",
      "--- Tweet 2 ---\n",
      "Date: None\n",
      "Text: @Th1s2ShallPass 우아앙ㅠㅠㅠㅠ 감사합니다\n",
      "뿌요뿌요처럼 터지더라구요...\n",
      "Engagement: 0.0 replies, 0.0 retweets, 0.0 likes, 11.0 views\n",
      "\n",
      "--- Tweet 3 ---\n",
      "Date: None\n",
      "Text: @harparr1 He is corrupt. Everybody in the Biden ministration is corrupt....\n",
      "Engagement: 0.0 replies, 0.0 retweets, 0.0 likes, 2.0 views\n",
      "\n",
      "--- Tweet 4 ---\n",
      "Date: None\n",
      "Text: @LangmanVince You're definitely not alone. There are plenty of obnoxious, adolescent-minded adults (MAGA/assholes) who are obsessed with physical appearances and ranking people accordingly....\n",
      "Engagement: 0.0 replies, 0.0 retweets, 0.0 likes, 16.0 views\n",
      "\n",
      "--- Tweet 5 ---\n",
      "Date: None\n",
      "Text: @FreetheFalcons @chwedivision @multifandomhomo @hottogojo If she wasn’t making that recognition, your point would be made by that quote. She refused to perform the WH because Biden’s complicit in geno...\n",
      "Engagement: 0.0 replies, 0.0 retweets, 4.0 likes, 145.0 views\n"
     ]
    }
   ],
   "source": [
    "def analyze_database():\n",
    "    # Connect to the existing database\n",
    "    con = duckdb.connect(\"nl2sql_agent/database/tweets1.duckdb\")\n",
    "    \n",
    "    # Get total row count\n",
    "    total_rows = con.execute(\"SELECT COUNT(*) FROM tweets\").fetchone()[0]\n",
    "    print(f\"\\nTotal rows in database: {total_rows:,}\")\n",
    "    \n",
    "    # Count NULL values for each column\n",
    "    null_counts_query = \"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) - COUNT(id) as id_nulls,\n",
    "        COUNT(*) - COUNT(text) as text_nulls,\n",
    "        COUNT(*) - COUNT(url) as url_nulls,\n",
    "        COUNT(*) - COUNT(epoch) as epoch_nulls,\n",
    "        COUNT(*) - COUNT(media) as media_nulls,\n",
    "        COUNT(*) - COUNT(retweetedTweet) as retweetedTweet_nulls,\n",
    "        COUNT(*) - COUNT(lang) as lang_nulls,\n",
    "        COUNT(*) - COUNT(rawContent) as rawContent_nulls,\n",
    "        COUNT(*) - COUNT(NULLIF(replyCount, 0)) as replyCount_nulls,\n",
    "        COUNT(*) - COUNT(NULLIF(retweetCount, 0)) as retweetCount_nulls,\n",
    "        COUNT(*) - COUNT(NULLIF(likeCount, 0)) as likeCount_nulls,\n",
    "        COUNT(*) - COUNT(NULLIF(quoteCount, 0)) as quoteCount_nulls,\n",
    "        COUNT(*) - COUNT(NULLIF(viewCount, 0)) as viewCount_nulls,\n",
    "        COUNT(*) - COUNT(location) as location_nulls,\n",
    "        COUNT(*) - COUNT(user) as user_nulls,\n",
    "        COUNT(*) - COUNT(date) as date_nulls\n",
    "    FROM tweets\n",
    "    \"\"\"\n",
    "    null_counts = con.execute(null_counts_query).fetchone()\n",
    "    \n",
    "    print(\"\\nNull/Empty value counts:\")\n",
    "    column_names = ['id', 'text', 'url', 'epoch', 'media', 'retweetedTweet', 'lang', \n",
    "                   'rawContent', 'replyCount', 'retweetCount', 'likeCount', 'quoteCount',\n",
    "                   'viewCount', 'location', 'user', 'date']\n",
    "    \n",
    "    for col, count in zip(column_names, null_counts):\n",
    "        percentage = (count / total_rows) * 100\n",
    "        print(f\"{col:<15} {count:>10,} ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    # Basic statistics for numeric columns\n",
    "    stats_query = \"\"\"\n",
    "    SELECT \n",
    "        'Engagement Metrics' as metric,\n",
    "        AVG(replyCount) as avg_replies,\n",
    "        MAX(replyCount) as max_replies,\n",
    "        AVG(retweetCount) as avg_retweets,\n",
    "        MAX(retweetCount) as max_retweets,\n",
    "        AVG(likeCount) as avg_likes,\n",
    "        MAX(likeCount) as max_likes,\n",
    "        AVG(viewCount) as avg_views,\n",
    "        MAX(viewCount) as max_views\n",
    "    FROM tweets\n",
    "    \"\"\"\n",
    "    stats = con.execute(stats_query).fetchone()\n",
    "    \n",
    "    print(\"\\nEngagement Statistics:\")\n",
    "    print(f\"Replies:  Avg: {stats[1]:,.1f}, Max: {stats[2]:,}\")\n",
    "    print(f\"Retweets: Avg: {stats[3]:,.1f}, Max: {stats[4]:,}\")\n",
    "    print(f\"Likes:    Avg: {stats[5]:,.1f}, Max: {stats[6]:,}\")\n",
    "    print(f\"Views:    Avg: {stats[7]:,.1f}, Max: {stats[8]:,}\")\n",
    "    \n",
    "    # Display 5 random rows\n",
    "    print(\"\\nRandom Sample of Tweets:\")\n",
    "    sample_query = \"\"\"\n",
    "    SELECT \n",
    "        text,\n",
    "        replyCount,\n",
    "        retweetCount,\n",
    "        likeCount,\n",
    "        viewCount,\n",
    "        date\n",
    "    FROM tweets \n",
    "    WHERE text IS NOT NULL\n",
    "    ORDER BY random() \n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    samples = con.execute(sample_query).fetchall()\n",
    "    \n",
    "    for i, row in enumerate(samples, 1):\n",
    "        print(f\"\\n--- Tweet {i} ---\")\n",
    "        print(f\"Date: {row[5]}\")\n",
    "        print(f\"Text: {row[0][:200]}...\")\n",
    "        print(f\"Engagement: {row[1]} replies, {row[2]} retweets, {row[3]} likes, {row[4]} views\")\n",
    "    \n",
    "    con.close()\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following script to download tweets from repo and extract them into a sql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def main():\n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 1: Clone the GitHub repo if it doesn't already exist\n",
    "    # --------------------------------------------------------------------\n",
    "    repo_url = \"https://github.com/sinking8/usc-x-24-us-election.git\"\n",
    "    repo_folder = \"usc-x-24-us-election\"\n",
    "\n",
    "    if not os.path.isdir(repo_folder):\n",
    "        print(f\"Cloning repo from {repo_url} into {repo_folder}...\")\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "    else:\n",
    "        print(f\"Repository folder '{repo_folder}' already exists. Skipping clone.\")\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 2: Set base_folder to the cloned repo\n",
    "    # --------------------------------------------------------------------\n",
    "    base_folder = repo_folder  # i.e., \"usc-x-24-us-election\"\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 3: Connect to (or create) the DuckDB database file\n",
    "    # --------------------------------------------------------------------\n",
    "    con = duckdb.connect(\"database/tweets.duckdb\")\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 4: Create the table if it does not exist\n",
    "    # --------------------------------------------------------------------\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tweets (\n",
    "        id VARCHAR,\n",
    "        text VARCHAR,\n",
    "        url VARCHAR,\n",
    "        epoch VARCHAR,\n",
    "        media VARCHAR,\n",
    "        retweetedTweet VARCHAR,\n",
    "        retweetedTweetID VARCHAR,\n",
    "        retweetedUserID VARCHAR,\n",
    "        id_str VARCHAR,\n",
    "        lang VARCHAR,\n",
    "        rawContent VARCHAR,\n",
    "        replyCount DOUBLE,\n",
    "        retweetCount DOUBLE,\n",
    "        likeCount DOUBLE,\n",
    "        quoteCount DOUBLE,\n",
    "        conversationId VARCHAR,\n",
    "        conversationIdStr VARCHAR,\n",
    "        hashtags VARCHAR,\n",
    "        mentionedUsers VARCHAR,\n",
    "        links VARCHAR,\n",
    "        viewCount DOUBLE,\n",
    "        quotedTweet VARCHAR,\n",
    "        in_reply_to_screen_name VARCHAR,\n",
    "        in_reply_to_status_id_str VARCHAR,\n",
    "        in_reply_to_user_id_str VARCHAR,\n",
    "        location VARCHAR,\n",
    "        cash_app_handle VARCHAR,\n",
    "        user VARCHAR,\n",
    "        date VARCHAR,\n",
    "        _type VARCHAR\n",
    "    );\n",
    "    \"\"\"\n",
    "    con.execute(create_table_query)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 5: Loop through part_* folders in the cloned repo and ingest CSV.GZ files\n",
    "    # --------------------------------------------------------------------\n",
    "    pattern = os.path.join(base_folder, \"part_*\", \"*.csv.gz\")\n",
    "    file_list = sorted(glob.glob(pattern))\n",
    "    \n",
    "    for csv_gz_file in file_list:\n",
    "        print(f\"Ingesting {csv_gz_file} ...\")\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO tweets\n",
    "        SELECT\n",
    "            NULLIF(id, '') AS id,\n",
    "            NULLIF(text, '') AS text,\n",
    "            NULLIF(url, '') AS url,\n",
    "            NULLIF(epoch, '') AS epoch,\n",
    "            NULLIF(media, '') AS media,\n",
    "            NULLIF(retweetedTweet, '') AS retweetedTweet,\n",
    "            NULLIF(retweetedTweetID, '') AS retweetedTweetID,\n",
    "            NULLIF(retweetedUserID, '') AS retweetedUserID,\n",
    "            NULLIF(id_str, '') AS id_str,\n",
    "            NULLIF(lang, '') AS lang,\n",
    "            NULLIF(rawContent, '') AS rawContent,\n",
    "            COALESCE(TRY_CAST(NULLIF(replyCount, '') AS DOUBLE), 0) AS replyCount,\n",
    "            COALESCE(TRY_CAST(NULLIF(retweetCount, '') AS DOUBLE), 0) AS retweetCount,\n",
    "            COALESCE(TRY_CAST(NULLIF(likeCount, '') AS DOUBLE), 0) AS likeCount,\n",
    "            COALESCE(TRY_CAST(NULLIF(quoteCount, '') AS DOUBLE), 0) AS quoteCount,\n",
    "            NULLIF(conversationId, '') AS conversationId,\n",
    "            NULLIF(conversationIdStr, '') AS conversationIdStr,\n",
    "            NULLIF(hashtags, '') AS hashtags,\n",
    "            NULLIF(mentionedUsers, '') AS mentionedUsers,\n",
    "            NULLIF(links, '') AS links,\n",
    "            COALESCE(TRY_CAST(NULLIF(regexp_extract(viewCount, 'count'': ''(\\d+)', 1), '') AS DOUBLE), 0) AS viewCount,\n",
    "            NULLIF(quotedTweet, '') AS quotedTweet,\n",
    "            NULLIF(in_reply_to_screen_name, '') AS in_reply_to_screen_name,\n",
    "            NULLIF(in_reply_to_status_id_str, '') AS in_reply_to_status_id_str,\n",
    "            NULLIF(in_reply_to_user_id_str, '') AS in_reply_to_user_id_str,\n",
    "            NULLIF(location, '') AS location,\n",
    "            NULLIF(cash_app_handle, '') AS cash_app_handle,\n",
    "            NULLIF(user, '') AS user,\n",
    "            NULLIF(date, '') AS date,\n",
    "            NULLIF(_type, '') AS _type\n",
    "        FROM read_csv_auto(\n",
    "            ?,\n",
    "            header=TRUE,\n",
    "            sample_size=-1,\n",
    "            all_varchar=TRUE,\n",
    "            ignore_errors=TRUE\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        con.execute(insert_query, [csv_gz_file])\n",
    "    \n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 6: Close the connection\n",
    "    # --------------------------------------------------------------------\n",
    "    print(\"All CSV.gz files have been ingested successfully.\")\n",
    "    con.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
