{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting data/part_11/may_july_chunk_201.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_202.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_203.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_204.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_205.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_206.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_207.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_208.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_209.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_210.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_211.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_212.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_213.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_214.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_215.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_216.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_217.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_218.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_219.csv.gz ...\n",
      "Ingesting data/part_11/may_july_chunk_220.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_221.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_222.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_223.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_224.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_225.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_226.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_227.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_228.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_229.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_230.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_231.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_232.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_233.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_234.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_235.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_236.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_237.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_238.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_239.csv.gz ...\n",
      "Ingesting data/part_12/may_july_chunk_240.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_241.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_242.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_243.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_244.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_245.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_246.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_247.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_248.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_249.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_250.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_251.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_252.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_253.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_254.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_255.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_256.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_257.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_258.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_259.csv.gz ...\n",
      "Ingesting data/part_13/may_july_chunk_260.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_261.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_262.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_263.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_264.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_265.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_266.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_267.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_268.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_269.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_270.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_271.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_272.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_273.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_274.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_275.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_276.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_277.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_278.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_279.csv.gz ...\n",
      "Ingesting data/part_14/may_july_chunk_280.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_281.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_282.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_283.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_284.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_285.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_286.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_287.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_288.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_289.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_290.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_291.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_292.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_293.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_294.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_295.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_296.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_297.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_298.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_299.csv.gz ...\n",
      "Ingesting data/part_15/may_july_chunk_300.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_301.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_302.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_303.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_304.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_305.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_306.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_307.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_308.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_309.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_310.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_311.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_312.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_313.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_314.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_315.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_316.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_317.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_318.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_319.csv.gz ...\n",
      "Ingesting data/part_16/may_july_chunk_320.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_321.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_322.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_323.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_324.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_325.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_326.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_327.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_328.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_329.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_330.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_331.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_332.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_333.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_334.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_335.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_336.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_337.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_338.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_339.csv.gz ...\n",
      "Ingesting data/part_17/may_july_chunk_340.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_341.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_342.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_343.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_344.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_345.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_346.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_347.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_348.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_349.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_350.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_351.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_352.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_353.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_354.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_355.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_356.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_357.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_358.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_359.csv.gz ...\n",
      "Ingesting data/part_18/may_july_chunk_360.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_361.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_362.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_363.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_364.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_365.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_366.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_367.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_368.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_369.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_370.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_371.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_372.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_373.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_374.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_375.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_376.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_377.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_378.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_379.csv.gz ...\n",
      "Ingesting data/part_19/may_july_chunk_380.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_381.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_382.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_383.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_384.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_385.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_386.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_387.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_388.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_389.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_390.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_391.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_392.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_393.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_394.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_395.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_396.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_397.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_398.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_399.csv.gz ...\n",
      "Ingesting data/part_20/may_july_chunk_400.csv.gz ...\n",
      "All CSV.gz files have been ingested successfully.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import duckdb\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    # Path to the top-level folder that contains part_1, part_2, etc.\n",
    "    base_folder = \"data/\"  # <-- Change this to your actual folder\n",
    "    \n",
    "    # Connect to (or create) the DuckDB database file\n",
    "    # This will create a file called \"tweets.duckdb\" in the current directory.\n",
    "    # Adjust the path if you want it somewhere else.\n",
    "    con = duckdb.connect(\"database/tweets.duckdb\")\n",
    "    \n",
    "    # Create the table (if it doesn't exist).\n",
    "    # Adjust data types as necessary for your use case:\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tweets (\n",
    "        id VARCHAR,\n",
    "        text VARCHAR,\n",
    "        url VARCHAR,\n",
    "        epoch VARCHAR,\n",
    "        media VARCHAR,\n",
    "        retweetedTweet VARCHAR,\n",
    "        retweetedTweetID VARCHAR,\n",
    "        retweetedUserID VARCHAR,\n",
    "        id_str VARCHAR,\n",
    "        lang VARCHAR,\n",
    "        rawContent VARCHAR,\n",
    "        replyCount DOUBLE,\n",
    "        retweetCount DOUBLE,\n",
    "        likeCount DOUBLE,\n",
    "        quoteCount DOUBLE,\n",
    "        conversationId VARCHAR,\n",
    "        conversationIdStr VARCHAR,\n",
    "        hashtags VARCHAR,\n",
    "        mentionedUsers VARCHAR,\n",
    "        links VARCHAR,\n",
    "        viewCount DOUBLE,\n",
    "        quotedTweet VARCHAR,\n",
    "        in_reply_to_screen_name VARCHAR,\n",
    "        in_reply_to_status_id_str VARCHAR,\n",
    "        in_reply_to_user_id_str VARCHAR,\n",
    "        location VARCHAR,\n",
    "        cash_app_handle VARCHAR,\n",
    "        user VARCHAR,\n",
    "        date VARCHAR,\n",
    "        _type VARCHAR\n",
    "    );\n",
    "    \"\"\"\n",
    "    con.execute(create_table_query)\n",
    "\n",
    "    # Loop through part_1, part_2, ... folders, grabbing *.csv.gz\n",
    "    # For example, if you have:\n",
    "    #   base_folder/part_1/something.csv.gz\n",
    "    #   base_folder/part_2/another.csv.gz\n",
    "    # etc.\n",
    "    pattern = os.path.join(base_folder, \"part_*\", \"*.csv.gz\")\n",
    "    file_list = sorted(glob.glob(pattern))\n",
    "    \n",
    "    for csv_gz_file in file_list:\n",
    "        print(f\"Ingesting {csv_gz_file} ...\")\n",
    "        # Insert data directly from CSV into the tweets table\n",
    "        # DuckDB can handle compressed CSV if the extension ends in .gz\n",
    "        # read_csv_auto(...) will infer schema automatically.\n",
    "        # Because we already created the table with a certain schema,\n",
    "        # we use ALL_VARCHAR=TRUE or align columns carefully.\n",
    "        # If you want to rely entirely on the schema we created, set ALL_VARCHAR=TRUE\n",
    "        # or ensure columns match 1-to-1 in the same order.\n",
    "        \n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO tweets\n",
    "        SELECT\n",
    "            NULLIF(id, '') AS id,\n",
    "            NULLIF(text, '') AS text,\n",
    "            NULLIF(url, '') AS url,\n",
    "            NULLIF(epoch, '') AS epoch,\n",
    "            NULLIF(media, '') AS media,\n",
    "            NULLIF(retweetedTweet, '') AS retweetedTweet,\n",
    "            NULLIF(retweetedTweetID, '') AS retweetedTweetID,\n",
    "            NULLIF(retweetedUserID, '') AS retweetedUserID,\n",
    "            NULLIF(id_str, '') AS id_str,\n",
    "            NULLIF(lang, '') AS lang,\n",
    "            NULLIF(rawContent, '') AS rawContent,\n",
    "            COALESCE(TRY_CAST(NULLIF(replyCount, '') AS DOUBLE), 0) AS replyCount,\n",
    "            COALESCE(TRY_CAST(NULLIF(retweetCount, '') AS DOUBLE), 0) AS retweetCount,\n",
    "            COALESCE(TRY_CAST(NULLIF(likeCount, '') AS DOUBLE), 0) AS likeCount,\n",
    "            COALESCE(TRY_CAST(NULLIF(quoteCount, '') AS DOUBLE), 0) AS quoteCount,\n",
    "            NULLIF(conversationId, '') AS conversationId,\n",
    "            NULLIF(conversationIdStr, '') AS conversationIdStr,\n",
    "            NULLIF(hashtags, '') AS hashtags,\n",
    "            NULLIF(mentionedUsers, '') AS mentionedUsers,\n",
    "            NULLIF(links, '') AS links,\n",
    "            COALESCE(TRY_CAST(NULLIF(regexp_extract(viewCount, 'count'': ''(\\d+)', 1), '') AS DOUBLE), 0) AS viewCount,\n",
    "            NULLIF(quotedTweet, '') AS quotedTweet,\n",
    "            NULLIF(in_reply_to_screen_name, '') AS in_reply_to_screen_name,\n",
    "            NULLIF(in_reply_to_status_id_str, '') AS in_reply_to_status_id_str,\n",
    "            NULLIF(in_reply_to_user_id_str, '') AS in_reply_to_user_id_str,\n",
    "            NULLIF(location, '') AS location,\n",
    "            NULLIF(cash_app_handle, '') AS cash_app_handle,\n",
    "            NULLIF(user, '') AS user,\n",
    "            NULLIF(date, '') AS date,\n",
    "            NULLIF(_type, '') AS _type\n",
    "        FROM read_csv_auto(\n",
    "            ?,\n",
    "            header=TRUE,\n",
    "            sample_size=-1,\n",
    "            all_varchar=TRUE,\n",
    "            ignore_errors=true\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        con.execute(insert_query, [csv_gz_file])\n",
    "    \n",
    "    print(\"All CSV.gz files have been ingested successfully.\")\n",
    "    con.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows in database: 20,000,016\n",
      "\n",
      "Null/Empty value counts:\n",
      "id                       1 (  0.0%)\n",
      "text                    33 (  0.0%)\n",
      "url                     70 (  0.0%)\n",
      "epoch                   73 (  0.0%)\n",
      "media                  100 (  0.0%)\n",
      "retweetedTweet          48 (  0.0%)\n",
      "lang                    95 (  0.0%)\n",
      "rawContent             126 (  0.0%)\n",
      "replyCount      15,983,111 ( 79.9%)\n",
      "retweetCount    17,530,827 ( 87.7%)\n",
      "likeCount       12,929,449 ( 64.6%)\n",
      "quoteCount      19,300,042 ( 96.5%)\n",
      "viewCount        2,300,627 ( 11.5%)\n",
      "location        18,234,419 ( 91.2%)\n",
      "user                   134 (  0.0%)\n",
      "date            20,000,012 (100.0%)\n",
      "\n",
      "Engagement Statistics:\n",
      "Replies:  Avg: 180,081,764,899.9, Max: 1.8050027949417231e+18\n",
      "Retweets: Avg: 452,974,972,781.4, Max: 1.8165403358264364e+18\n",
      "Likes:    Avg: 137,135,678,736.6, Max: 1.814654783417926e+18\n",
      "Views:    Avg: 1,556.6, Max: 439,665,677.0\n",
      "\n",
      "Random Sample of Tweets:\n",
      "\n",
      "--- Tweet 1 ---\n",
      "Date: None\n",
      "Text: El expresidente Donald Trump ha obtenido mÃ¡s de los votos de delegados requeridos para ser el nominado republicano para correr por la presidencia de Estados Unidos. \n",
      "@TelemundoAZ estÃ¡ en Milwaukee....\n",
      "Engagement: 0.0 replies, 0.0 retweets, 1.0 likes, 35.0 views\n",
      "\n",
      "--- Tweet 2 ---\n",
      "Date: None\n",
      "Text: @MARINESARALDI MAGA...\n",
      "Engagement: 0.0 replies, 0.0 retweets, 0.0 likes, 6.0 views\n",
      "\n",
      "--- Tweet 3 ---\n",
      "Date: None\n",
      "Text: @WhistleblowerLF â€œI Am Vanessa GuillÃ©n Actâ€ revamps military investigations | The Texas Tribune\n",
      "\n",
      "\"President Biden signed the National Defense Authorization Act, enacting provisions of the 'I Am Vaness...\n",
      "Engagement: 0.0 replies, 0.0 retweets, 0.0 likes, 3.0 views\n",
      "\n",
      "--- Tweet 4 ---\n",
      "Date: None\n",
      "Text: @DontLookBack45 Hi Sam.. Good Morning ðŸŒž...\n",
      "Engagement: 0.0 replies, 0.0 retweets, 1.0 likes, 24.0 views\n",
      "\n",
      "--- Tweet 5 ---\n",
      "Date: None\n",
      "Text: @catturd2 https://t.co/8aVCLt5sb3...\n",
      "Engagement: 0.0 replies, 0.0 retweets, 0.0 likes, 2.0 views\n"
     ]
    }
   ],
   "source": [
    "def analyze_database():\n",
    "    # Connect to the existing database\n",
    "    con = duckdb.connect(\"database/tweets.duckdb\")\n",
    "    \n",
    "    # Get total row count\n",
    "    total_rows = con.execute(\"SELECT COUNT(*) FROM tweets\").fetchone()[0]\n",
    "    print(f\"\\nTotal rows in database: {total_rows:,}\")\n",
    "    \n",
    "    # Count NULL values for each column\n",
    "    null_counts_query = \"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) - COUNT(id) as id_nulls,\n",
    "        COUNT(*) - COUNT(text) as text_nulls,\n",
    "        COUNT(*) - COUNT(url) as url_nulls,\n",
    "        COUNT(*) - COUNT(epoch) as epoch_nulls,\n",
    "        COUNT(*) - COUNT(media) as media_nulls,\n",
    "        COUNT(*) - COUNT(retweetedTweet) as retweetedTweet_nulls,\n",
    "        COUNT(*) - COUNT(lang) as lang_nulls,\n",
    "        COUNT(*) - COUNT(rawContent) as rawContent_nulls,\n",
    "        COUNT(*) - COUNT(NULLIF(replyCount, 0)) as replyCount_nulls,\n",
    "        COUNT(*) - COUNT(NULLIF(retweetCount, 0)) as retweetCount_nulls,\n",
    "        COUNT(*) - COUNT(NULLIF(likeCount, 0)) as likeCount_nulls,\n",
    "        COUNT(*) - COUNT(NULLIF(quoteCount, 0)) as quoteCount_nulls,\n",
    "        COUNT(*) - COUNT(NULLIF(viewCount, 0)) as viewCount_nulls,\n",
    "        COUNT(*) - COUNT(location) as location_nulls,\n",
    "        COUNT(*) - COUNT(user) as user_nulls,\n",
    "        COUNT(*) - COUNT(date) as date_nulls\n",
    "    FROM tweets\n",
    "    \"\"\"\n",
    "    null_counts = con.execute(null_counts_query).fetchone()\n",
    "    \n",
    "    print(\"\\nNull/Empty value counts:\")\n",
    "    column_names = ['id', 'text', 'url', 'epoch', 'media', 'retweetedTweet', 'lang', \n",
    "                   'rawContent', 'replyCount', 'retweetCount', 'likeCount', 'quoteCount',\n",
    "                   'viewCount', 'location', 'user', 'date']\n",
    "    \n",
    "    for col, count in zip(column_names, null_counts):\n",
    "        percentage = (count / total_rows) * 100\n",
    "        print(f\"{col:<15} {count:>10,} ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    # Basic statistics for numeric columns\n",
    "    stats_query = \"\"\"\n",
    "    SELECT \n",
    "        'Engagement Metrics' as metric,\n",
    "        AVG(replyCount) as avg_replies,\n",
    "        MAX(replyCount) as max_replies,\n",
    "        AVG(retweetCount) as avg_retweets,\n",
    "        MAX(retweetCount) as max_retweets,\n",
    "        AVG(likeCount) as avg_likes,\n",
    "        MAX(likeCount) as max_likes,\n",
    "        AVG(viewCount) as avg_views,\n",
    "        MAX(viewCount) as max_views\n",
    "    FROM tweets\n",
    "    \"\"\"\n",
    "    stats = con.execute(stats_query).fetchone()\n",
    "    \n",
    "    print(\"\\nEngagement Statistics:\")\n",
    "    print(f\"Replies:  Avg: {stats[1]:,.1f}, Max: {stats[2]:,}\")\n",
    "    print(f\"Retweets: Avg: {stats[3]:,.1f}, Max: {stats[4]:,}\")\n",
    "    print(f\"Likes:    Avg: {stats[5]:,.1f}, Max: {stats[6]:,}\")\n",
    "    print(f\"Views:    Avg: {stats[7]:,.1f}, Max: {stats[8]:,}\")\n",
    "    \n",
    "    # Display 5 random rows\n",
    "    print(\"\\nRandom Sample of Tweets:\")\n",
    "    sample_query = \"\"\"\n",
    "    SELECT \n",
    "        text,\n",
    "        replyCount,\n",
    "        retweetCount,\n",
    "        likeCount,\n",
    "        viewCount,\n",
    "        date\n",
    "    FROM tweets \n",
    "    WHERE text IS NOT NULL\n",
    "    ORDER BY random() \n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    samples = con.execute(sample_query).fetchall()\n",
    "    \n",
    "    for i, row in enumerate(samples, 1):\n",
    "        print(f\"\\n--- Tweet {i} ---\")\n",
    "        print(f\"Date: {row[5]}\")\n",
    "        print(f\"Text: {row[0][:200]}...\")\n",
    "        print(f\"Engagement: {row[1]} replies, {row[2]} retweets, {row[3]} likes, {row[4]} views\")\n",
    "    \n",
    "    con.close()\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_database()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
